{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f01c00",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d231366",
   "metadata": {},
   "source": [
    "Written by: [Thomas Flynn](https://www.bnl.gov/staff/tflynn) | _Assistant Computational Scientist, Computational Science Initiative, Brookhaven National Laboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcaef6f",
   "metadata": {},
   "source": [
    "This notebook will cover the basics of multi-layer neural networks. First, we recall some of the material from the previous tutorials on classification and regression. Then we'll have an introduction to neural networks, focusing on a particular case - the multi-layer perceptron (MLP). After walking through a detailed numerical example, we'll test out the MLP on several example datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0991cc7",
   "metadata": {},
   "source": [
    "# 1. Review of classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ca18f",
   "metadata": {},
   "source": [
    "## Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff914e",
   "metadata": {},
   "source": [
    "One of the fundamental machine learning tasks is learning a mapping from an input to a discrete set of classes. This is known as classification. For example, finding a mapping that takes an image and assigning a label to it from the classes {cat, dog} is a classification problem. The mapping itself is called a classifier.\n",
    "\n",
    "To build up to neural networks, we'll first go over some of the approaches covered in earlier lectures, including logistic regression and decision trees. We'll do this in the context of a toy dataset from the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc896d",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d89a4",
   "metadata": {},
   "source": [
    "First, we load up an example dataset. This will be used to evaluate logistic regression and also to compare with other classification approaches. We'll use the digits dataset, which consists of about 1800 8x8 grayscale images of digits, and the corresponding label indicating the digit (a number from 0 through 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91986b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "example_dataset = load_digits()\n",
    "\n",
    "X = example_dataset[\"data\"]\n",
    "y = example_dataset[\"target\"]\n",
    "\n",
    "df = pd.DataFrame(zip(X,y),columns = [\"X\",\"y\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88d0c",
   "metadata": {},
   "source": [
    "We canuse matplotlib's `imshow` function to view the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "idx = 0\n",
    "\n",
    "plt.imshow(X[idx].reshape(8,8))\n",
    "plt.title(\"Label: {}\".format(y[idx]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6305b4c",
   "metadata": {},
   "source": [
    "Now we'll train a logistic regression model on this dataset. For this, we'll use the built in `LogisticRegression` model type from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=123)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "preds_test = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on testing data: {}\".format( accuracy_score(y_test,preds_test)) )\n",
    "\n",
    "pd.DataFrame(zip(X_test,y_test,preds_test),columns = [\"X\",\"True y\", \"Predicted y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d93a02",
   "metadata": {},
   "source": [
    "# Underneath the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d32c4",
   "metadata": {},
   "source": [
    "Logistic regression works by finding a linear separator of the classes. Geometrically, this can be thought of as a plane in the input space, which separates the space into everything above and everything below the plane.\n",
    "<img src=\"img/linearly_separable_4.png\">\n",
    "The training phase of logistic regressing finds the coefficients that represent this plane. In scikit-learn, this is represented as two data attributes: a vector `logref.coef_` and a numerical offset `logref.interept_`.\n",
    "\n",
    "At inference time, given an item to classify, we first transform the features with the coefficient matrix to get a vector of scores for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "raw_scores = logreg.coef_ @ X_test[5] + logreg.intercept_\n",
    "\n",
    "plt.plot(raw_scores,'+',color='b')\n",
    "plt.xticks(ticks=range(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67cb798",
   "metadata": {},
   "source": [
    "The softmax function can be used to map the raw scores into a probability distribution over the labels, as shown below. We have also indicated the true class for this example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf270269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "scores = softmax(raw_scores)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "plt.plot(scores,'+', color='b')\n",
    "plt.axvline(y_test[5],color='g',label=\"True class\")\n",
    "plt.axvline(preds_test[5],linestyle='--',color='r',label=\"Predicted class\")\n",
    "plt.legend()\n",
    "plt.xticks(ticks=range(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4083c0",
   "metadata": {},
   "source": [
    "To be sure, let's look at test example 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "plt.imshow(X_test[5].reshape(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba309598",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec1144",
   "metadata": {},
   "source": [
    "We also saw decision trees. These can be used for classification as well, and are just as simple to access in scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=100, random_state=123)\n",
    "\n",
    "classifier.fit(X=X_train, y=y_train)\n",
    "\n",
    "preds_test = classifier.predict(X_test)\n",
    "\n",
    "print(\"Overall accuracy: {}\".format(accuracy_score(y_test,preds_test)))\n",
    "\n",
    "pd.DataFrame(zip(X_test,y_test,preds_test),columns = [\"X\",\"True y\", \"Predicted y\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf75bf",
   "metadata": {},
   "source": [
    "# 2. Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b14772",
   "metadata": {},
   "source": [
    "## Structure of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2e860",
   "metadata": {},
   "source": [
    "Neural network models process their inputs by passing them through a number of transformations before producing their final outputs. We'll focus on one of the simplest classes of neural networks - multi layer perceptrons - for this tutorial. A multilayer perceptron, or MLP, model is made up of a number of layers. Each layer computes a transformation of the data produced at the previous layer. You can keep in mind the following picture:\n",
    "<center><img src=\"img/multi-layer-perceptron-in-tensorflow.png\" alt=\"Logic gate symbols, as typically used in electronics\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101dddcf",
   "metadata": {},
   "source": [
    "Above, we can see three different parts of the network. On the left the input layer. This represents the input to your network. For example, the image you want to classify, or the various plant features in the iris dataset. The right shows the output layer. In a classification scenario, the nodes of the output layer have the scores for each possible class.\n",
    "In between, we have the hidden layers. These represent the intermediate states of the  computation. The idea is that by passing the input through multiple stages of processing, the final hidden layer might be a more useful representation for classification compared to a raw input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04cb68",
   "metadata": {},
   "source": [
    "Let us get into more details using an example of a network with 1 hidden layer. In this example, we will define a network that classifies an input vector, assigning it to one of two classes. We will assume the input is two dimensional. The output will be a single number.\n",
    "\n",
    "Mathematically, the model is defined using a set of weight matrices and bias vectors. Each layer has one weight matrix and one bias vector. We will denote these as $w_1$ and $w_2$ and $b_1$ and $b_2$.\n",
    "\n",
    "Another important component of the network is the activation function. We denote this by $\\sigma$. In the example below we use the sigmoid function (also known as the logistic function):\n",
    "$$\\sigma(u) = \\frac{1}{1+e^{-u}}$$\n",
    "\n",
    "As a reminder, here is how we may write the sigmoid function in python, along with a plot of how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151afe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "grid = np.linspace(-6, 6, 100)\n",
    "ax.plot(grid, sigmoid(grid))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$\\sigma(x)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7f999",
   "metadata": {},
   "source": [
    "Given these definitions, we are now ready to describe the neural network model. Given an input $x$, the value at the hidden layer is\n",
    "$$h = \\sigma( w_1 x + b_1)$$\n",
    "and the value at the output layer is\n",
    "$$y = w_2 h + b_2$$\n",
    "Overall, the mapping from the inputs to the outputs is\n",
    "$$y = w_2 \\sigma(w_1 x + b_1) + b_2$$\n",
    "Below, we implement this network in python, using some predfined weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c510c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array([[2., 2.],\n",
    "               [2., 2.]])\n",
    "\n",
    "b1 = np.array([-3,\n",
    "               -1])\n",
    "\n",
    "w2 = np.array([\n",
    "               [-2.,2.]])\n",
    "b2 = np.array([\n",
    "               -0.7])\n",
    "\n",
    "def simple_mlp(x):\n",
    "    h = sigmoid(w1@x + b1)\n",
    "    \n",
    "    out = w2@h + b2\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d246560",
   "metadata": {},
   "source": [
    "Let's see the output of this network at a few inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb66882",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [ [0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1] ]\n",
    "\n",
    "for v in vals:\n",
    "    out = simple_mlp(v)\n",
    "    print(\"For input {} the output is {}\".format(v,out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8460b2",
   "metadata": {},
   "source": [
    "Since we're using this network for classification, we'll assign the class based on the sign of the output. Non-positive outputs are mapped to class 0, while positive outputs get class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vals:\n",
    "    out = simple_mlp(v)\n",
    "    c = (1 if out >0 else 0) \n",
    "    print(\"For input {} the output class is {}\".format(v,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae32c0",
   "metadata": {},
   "source": [
    "Does this input/output mapping seem familiar to you? As we shall see below, this example highlights the ability of multi-layer neural networks to have non-linear decision boundaries, a property which sets these models apart from simpler ones like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbfaa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probing the network\n",
    "\n",
    "To gain some insight into how this neural network works, we can look at it's decision boundaries. This is a visual display of how the input space is partitioned into different classes by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2)) \n",
    "\n",
    "sz = 50\n",
    "i1 = np.linspace(-2,2,sz)\n",
    "\n",
    "pts = np.zeros((sz*sz,2))\n",
    "vals = np.zeros(sz*sz)\n",
    "\n",
    "for (idx, (x,y)) in enumerate(product(i1,i1)):\n",
    "    for j in range(sz):\n",
    "        v = simple_mlp(np.array([x,y]))\n",
    "        vals[idx] = (1 if v> 0 else 0)\n",
    "        pts[idx] = ([x,y])\n",
    "\n",
    "plt.plot(1,1,'+',c='black')\n",
    "plt.plot(0,0,'+',c='black')\n",
    "plt.plot(1,0,'+',c='black')\n",
    "plt.plot(0,1,'+',c='black')\n",
    "\n",
    "plt.scatter( pts[vals > 0][:,0], pts[vals>0][:,1],marker='.',label=\"Class 1\" )\n",
    "plt.scatter( pts[vals <= 0][:,0], pts[vals<=0][:,1],marker='.',label=\"Class 0\" )\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bb51d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not stick to logistic regression?\n",
    "Why go through all of the trouble of using an MLP if a logistic regression model works just fine? Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890ffa5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = [ [0, 0],\n",
    "      [0, 1],\n",
    "      [1, 0],\n",
    "      [1, 1] ]\n",
    "\n",
    "y = [ 0,\n",
    "      1,\n",
    "      1,\n",
    "      0]\n",
    "\n",
    "logreg = LogisticRegression(random_state=123)\n",
    "\n",
    "logreg.fit(X, y)\n",
    "\n",
    "preds_test = logreg.predict(X)\n",
    "\n",
    "print(\"Overall accuracy: {}\".format(accuracy_score(y,preds_test)))\n",
    "\n",
    "pd.DataFrame(zip(X,y,preds_test),columns = [\"X\",\"True y\", \"Predicted y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d408e7",
   "metadata": {},
   "source": [
    "We see that it only gets 50% right. This is because the decision boundary of a logistic regression model is always linear. There is no setting of coefficients and biases the will make such a linear model be able to separate the data (0,1) and (1,0) from (0,0) and (1,1).\n",
    "\n",
    "Intuitively, the benefit of using multiple layers, as in the case of the MLP we defined above, is that the earlier layers can transform the data such that they corresponding hidden representations are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd5d78",
   "metadata": {},
   "source": [
    "## Using scikit-learn's MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d558e",
   "metadata": {},
   "source": [
    "For the MLP example above, we hardcoded the weights and biases. In practice, we'd use an optimization procedure to find the best set of parameters, instead of setting them by hand. Let's see how we could do that using scikit learn in the context of the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = [ [0,0],\n",
    "      [0,1],\n",
    "      [1,0],\n",
    "      [1,1] ]\n",
    "\n",
    "y = [ 0,\n",
    "      1, \n",
    "      1,\n",
    "      0]\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(2),solver='lbfgs', activation='logistic',random_state=0)\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "preds_test = clf.predict(X)\n",
    "\n",
    "pd.DataFrame(zip(X,y,preds_test),columns = [\"X\",\"True y\", \"Predicted y\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694dfaf",
   "metadata": {},
   "source": [
    "Nice! Just as we did with our `simple_mlp` implementation, we can probe the decision boundary of the MLP we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafa238",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2)) \n",
    "\n",
    "sz = 50\n",
    "i1 = np.linspace(-2,2,sz)\n",
    "\n",
    "pts = np.zeros((sz*sz,2))\n",
    "vals = np.zeros(sz*sz)\n",
    "\n",
    "for (idx, (x,y)) in enumerate(product(i1,i1)):\n",
    "    for j in range(sz):\n",
    "        v = clf.predict(np.array([[x,y]]))\n",
    "        vals[idx] = v[0]\n",
    "        pts[idx] = ([x,y])\n",
    "\n",
    "plt.plot(1,1,'+',c='black')\n",
    "plt.plot(0,0,'+',c='black')\n",
    "plt.plot(1,0,'+',c='black')\n",
    "plt.plot(0,1,'+',c='black')\n",
    "\n",
    "plt.scatter( pts[vals > 0][:,0], pts[vals>0][:,1],marker='.',label=\"Class 1\" )\n",
    "plt.scatter( pts[vals <= 0][:,0], pts[vals<=0][:,1],marker='.',label=\"Class 0\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2247a53",
   "metadata": {},
   "source": [
    "There will probably be some subtle differences between the function learned by scikit-learn and the one we defined, but the classes assigned to the instances in our dataset should be the same.\n",
    "\n",
    "Next, let's try out MLP models on some of the toy datasets in scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f4556",
   "metadata": {},
   "source": [
    "## Example #1: Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = load_digits()\n",
    "\n",
    "X = example_dataset[\"data\"]\n",
    "y = example_dataset[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=123)\n",
    "\n",
    "clf = MLPClassifier(alpha=1,hidden_layer_sizes=(500), activation='logistic',random_state=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "preds_test = clf.predict(X_test)\n",
    "\n",
    "print(\"Overall accuracy: {}\".format(accuracy_score(y_test,preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde0bc7",
   "metadata": {},
   "source": [
    "## Example #2: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da918f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "example_dataset = load_iris()\n",
    "\n",
    "X = example_dataset[\"data\"]\n",
    "y = example_dataset[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=123)\n",
    "\n",
    "clf = MLPClassifier(alpha=1,hidden_layer_sizes=(500), activation='logistic', max_iter=1000, random_state=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "preds_test = clf.predict(X_test)\n",
    "\n",
    "print(\"Overall accuracy: {}\".format(accuracy_score(y_test,preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a904cf",
   "metadata": {},
   "source": [
    "## Example #3: Materials science data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecb5d2",
   "metadata": {},
   "source": [
    "As a final example, let's revisit the coordination number dataset from the RandomForests tutorial, which comes from the paper [1]. First, we load the data, fetching it from the web (this might take a few seconds.)\n",
    "\n",
    "[1] S. B. Torrisi, M. R. Carbone, B. A. Rohr, J. H. Montoya, Y. Ha, J. Yano, S. K. Suram & L. Hung. [Random forest machine learning models for interpretable X-ray absorption near-edge structure spectrum-property relationships.](https://www.nature.com/articles/s41524-020-00376-6) npj Comput. Mater. 6, 109 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89313b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "url = \"https://s3.amazonaws.com/publications.matr.io/4/deployment/data/files/spectral_data/Ti_XY.json\"\n",
    "r = requests.get(url)\n",
    "text = r.text.split(\"\\n\")\n",
    "data = [json.loads(xx) for xx in text[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee2143",
   "metadata": {},
   "source": [
    "Next, we create inputs and targets from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "e_grid = data[0][\"E\"]\n",
    "spectra = np.array([\n",
    "    dat[\"mu\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])\n",
    "coordinations = np.array([\n",
    "    dat[\"coordination\"] for dat in data\n",
    "    if dat[\"one_hot_coord\"] is not None\n",
    "])\n",
    "scaler = StandardScaler()\n",
    "spectra = scaler.fit_transform(spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f642b",
   "metadata": {},
   "source": [
    "From here, we can use the same workflow as the other classification examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47469420",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spectra, coordinations, test_size=0.1)\n",
    "\n",
    "clf = MLPClassifier(alpha=1,hidden_layer_sizes=(200), activation='relu',random_state=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "preds_test = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038667a",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "- We didn't go into details on data scaling or normalization, but neural network training often benefits from having the data normalized, so that each coordinate has mean zero and unit variance. In Scikit-learn, this is handled by the `StandardScaler` class. Try commenting out the `spectra = scaler(...` line and see how performance changes.\n",
    "\n",
    "- Experiment with different settings for `hidden_layer_sizes`. Try using some other values for hidden layer size, including smaller (e.g. 50 or 100) and larger (e.g. 400, 800). You can also pass multiple numbers, separated by commas, to have multiple hidden layers.\n",
    "\n",
    "- Using the digits dataset, compare the accuracy of decision trees, a MLP, and logistic regression. Is there any clear winner?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
