{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX_9BuPB_hA-"
      },
      "source": [
        "# Day 5: Introduction to Deep Neural Networks!\n",
        "\n",
        "In this tutorial, we're going to go over the fundimentals of Neural Networks, specifically focusing on **Dense Neural Networks**. (If you're interested in Convolutional Neural Networks, don't fret! You'll probably enjoy the next lecture ;) )\n",
        "\n",
        "Neural Networks as a whole are an _incredibly_ broad and complex topic. As we only have half a day today, we're not going to be able to cover much more than the basic ideas and techniques, but the hope is that this provides a stable base for you to continue building your knowledge on top of. If you don't itend to continue much farther down the machine learning rabbit hole, we hope that at the very least, this can serve to de-mystify neural networks/machine learning as a whole :)\n",
        "\n",
        "**Learning Objectives:**\n",
        "* Understand Vectors, Matricies, and Dot Product\n",
        "* Understand how a single layer perceptron works, along with batching and gradient descent.\n",
        "* Learn how to build and train a simple Deep Neural Network (Multi-Layer Perceptron - MLP) using the TensorFlow/Keras deep learning python framework\n",
        "* Learn about different optimizers beyond (Stochastic) Gradient Descent and touch on when you might want to use them\n",
        "* Touch on a few key \"Hyperparameters\" that you, the human, can and will need to optimize when designing a neural network (Learning Rate, Batch Size, etc.)\n",
        "\n",
        "\n",
        "**Other Resources**\n",
        "* a\n",
        "* b\n",
        "* c\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o65_IgEgLtT2",
        "outputId": "dcc7a2a5-b1b4-4226-b754-00a376330aeb"
      },
      "outputs": [],
      "source": [
        "# Run Me Ahead of time!\n",
        "from sklearn.datasets import fetch_openml\n",
        "jet_tagging_data = fetch_openml('hls4ml_lhc_jets_hlf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJdBSPiRLtT3",
        "tags": []
      },
      "source": [
        "## Notation, Terms, and other background info\n",
        "In this notebook, you'll likely see some new symbols, terminology, and other topics that you haven't come across before. Don't be afraid!\n",
        "\n",
        "Most of the math symbols you see here simply function as *shorthand* for different math concepts, so that we can write an equation using these concepts without needing to define \"generic\" variables or functions every time we reference them. We've compiled a list of these terms here for you to reference when needed, but at least skimming through these before continuing is reccomended.\n",
        "\n",
        "Additionally, there are some math concepts that are useful to know about/have a basic understanding of, as they're used quite heavily in this notebook (and Neural Networks/Machine Learning in general!)\n",
        "\n",
        "We're going to assume that you have a baseline level of knowledge regarding some math concepts, but if you run into something that you don't understand (regardless of if it's defined here or not!), please let us know and we'll be happy to explain it! Additionally, some definitions of terms might include Machine Learning concepts that are covered later in the notebook (e.g. *Activation Functions*), we'll include another list of terms/symbols that are covered as part of this notebook at the end, so for now don't worry too much if you run into one of these terms.\n",
        "\n",
        "## Math Concepts\n",
        "\n",
        "###  Vectors & Scalars\n",
        "a **vector** is a kind of variable that not only has a value (aka *magnitude*), but also a *direction*. One common way to represent and define vectors that we'll use is by stating the change along each axis (the *components*) the vector travels in (in 2D, this would be $x$ and $y$ *components*, in 3D, the $x$, $y$ and $z$ *components* ).\n",
        "\n",
        "This can be written as $\\vec{v} = (x,y)$ _or_ as $\\boldsymbol{v} = (x,y)$ (note the second $a$ is bolded). Sometimes, the *components* are written stacked vertically such as $\\vec{v} = \\begin{pmatrix} x\\\\ y\\\\ \\end{pmatrix}$\n",
        "\n",
        "In this format, calculating the *magnitude* of a vector, written as $| \\vec{v} |$, is done by applying the pythagorean theorm!\n",
        "\n",
        "<img src=\"img/vector_components.png\" alt=\"Plot labeling the components of a vector, plus the definition for mangitude\" style=\"background-color:white; width:400px;\" />\n",
        "\n",
        "* You can add or subtract vectors to/from vectors, this will produce a vector as a result ( the *resultant*)\n",
        "* Multiplication of a Vector with a Scalar produces a vector\n",
        "* Multiplication of Vectors with Vectors can produce either:\n",
        "    * Another Vector, by calculating the *Cross Product*: $\\vec{v} \\times \\vec{b} = \\vec{vb}$\n",
        "    * a *Scalar*, by calculating the *Dot Product*: $\\vec{v} \\cdot \\vec{b} = vb$\n",
        "* You cannot divide with vectors\n",
        "    \n",
        "a **scalar** is a normal value, without a direction. Scalars are considered to only have a *magnitude*. Most math you know and have done throughout school has been **scalar** math :)\n",
        "\n",
        "\n",
        "### Matrix/Matrices\n",
        "a **Matrix** is a _structured_ table/an array of numbers. Location of each element matters, and they can be **1 or more dimensions** in shape. Matricies are generally labeled as capital/uppercase letters, and are represented with it's elements placed in a rectangular grid, encapsulated by large square brackets $[  ]$ or parenthesis $(  )$  A Matrix with $m$ rows and $n$ columns can be written as    \n",
        "$$\n",
        "A = [a_{m,n}] = \\begin{bmatrix}\n",
        "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\\n",
        "  \\vdots & \\ddots & \\vdots\\\\\n",
        "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\\n",
        "  \\vdots & \\ddots & \\vdots\\\\\n",
        "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "So if we set $m=4$ and $n=2$\n",
        "$$\n",
        "A = [a_{4,2}] = \\begin{bmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Where ${a}_{i,j}$ (no brackets/parentheses) refers to a single element in the matrix, where $i$ refers to the row and $j$ refers to the column\n",
        "\n",
        "* Sometimes, we'll talk about *transposing* a matrix, which means we flip it along it's diagonal axis! When transposing a matrix, we swap the values of $m$ and $n$ and move the elements of the matrix to match. We refer to the transposed version of a matrix by adding a superscript of $T$ to it. For example\n",
        "<center><img src=\"img/Matrix_transpose.gif\" alt=\"Animation of a matrix being transposed\"/></center>\n",
        "$$\n",
        "A = [a_{3,2}] = \\begin{bmatrix}\n",
        "  1 & 2\\\\\n",
        "  3 & 4\\\\\n",
        "  5 & 6\\\\  \n",
        "\\end{bmatrix} \\quad  A^{T} = [a^{T}_{2,3}] = \\begin{bmatrix}\n",
        "  1 & 3 & 5 \\\\\n",
        "  2 & 4 & 6 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "* A 1-Dimensional matrix (where either $n=1$ or $m=1$) can also be called a **row vector** (when shape = $m \\times 1$) or a **column vector** (when shape = $1 \\times n$), and transposing one kind turns it into the other. These can be _treated_ like vectors (such as when you're performing certain operations), but are still matricies. A key point is that a *vector* **can not** be transposed, but a *matrix* can. (Yes this is confusing, this distinction won't come up today, but is important with Linear Algebra in general)\n",
        "$$\n",
        "A = [a_{4,1}] = \\begin{bmatrix}\n",
        "  1 \\\\\n",
        "  2 \\\\\n",
        "  3 \\\\\n",
        "  4 \\\\\n",
        "  \\end{bmatrix} \\quad B = [b_{1,4}] = \\begin{bmatrix}\n",
        "  5 & 6 & 7 & 8\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* **Importantly,** you can/will see some matricies represented in the form of a *column vector*, functioning as a ***table*** of vectors. You have to be careful in this distinction, as it's shorthand that needs to be expanded before performing operations on it as a matrix. For example, if we have a matrix $A = [a_{4,3}]$, we can write it as a table of 2-Dimensional vectors $\\vec{a_n} = (x,y)$ like so:\n",
        "\n",
        "$$\n",
        "A = [a_{4,2}] = \\begin{bmatrix}\n",
        "  \\vec{a_1} \\\\\n",
        "  \\vec{a_2} \\\\\n",
        "  \\vec{a_3} \\\\\n",
        "  \\vec{a_4} \\\\\n",
        "  \\end{bmatrix} = \\begin{bmatrix}\n",
        "  {a_1}_x & {a_1}_y\\\\\n",
        "  {a_2}_x & {a_2}_y\\\\\n",
        "  {a_3}_x & {a_3}_y\\\\\n",
        "  {a_4}_x & {a_4}_y\\\\\n",
        "  \\end{bmatrix} = \\begin{bmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Dot Product\n",
        "The dot product of two matricies is simply the sum of each corresponding element multiplied together, defined as such:\n",
        "\n",
        "$ \\mathbf a \\cdot \\mathbf b = \\sum_{i=1}^n a_i b_i = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n $\n",
        "\n",
        "\n",
        "\n",
        "***Variable/Function Definitions - ML/Notebook Specific***\n",
        "Most of these symbols are common across Machine Learning/Neural Network literature, but there still might be slight variations. For the purposes of this notebook, these variables are defined as follows\n",
        "\n",
        "Dataset Related Variables:\n",
        "* $\\vec{x}$ - An **unmodified** *input* vector, these make up the inputs to our neural network\n",
        "* $\\vec{x_n}$ - pronounced \"*x n*\", e.g. \"x 1\", \"x 2\", etc. - a specific input matrix (feature), e.g. $\\vec{x_1}$, $\\vec{x_2}$, etc. (this also applies to other forms of $\\vec{x}$, $\\vec{y}$, and $\\vec{w}$ such as $\\vec{x'}$ and $\\vec{\\hat{y}}$ )\n",
        "* $\\vec{x'}$ - pronounced *\"x prime\"* - Generally a modified value of x (derivitive of, though not speciifcally in the Calculus sense :) ), usually referring to the output of an intermediete step during pre-processing, an operation within a *hidden layer* of a neural network, or the output of the *hidden layer* itself\n",
        "* $\\vec{y}$ - A truth vector, containing whatever the _actual truth values_ that the neural network is trying to learn to replicate.\n",
        "* $\\vec{\\hat{y}}$ - pronounced *Y Hat* - the **modified**/**final** output vector of our neural network, usually after we apply an *activation function*\n",
        "* ${X}$ - (Capital/Uppercase X) - the _input_ matrix for our neural network, comprised of all input vectors ($x$)\n",
        "* $\\hat{Y}$ - (Capital/Uppercase Y Hat) - the _output_ matrix for our neural network, comprised of all output vectors ($\\hat{y}$) from our neutral network.\n",
        "\n",
        "Neural Network Related Variables/Functions:\n",
        "* $\\vec{w}$ - a *weight* vector, a _learned parameter_ that a neural network learns during the training process.  \n",
        "* $b$ - a *bias* scalar, a _learned parameter_ that a neural network learns during the training process.\n",
        "* $g(...)$ - an *activation function*, used to add *non linear behavior* to the network. This can represent one of multiple different functions (such as Step, ReLU, Sigmoid, TanH, etc. see the \"Activation Functions\" link in *Other Resources* above!), depending on the context and architecture of a given neural network.\n",
        "\n",
        "\n",
        "\n",
        "***Math Symbols/Definitions***\n",
        "This notation is pretty general and shared across most math literature you'll find, but it's important to check specific meanings regardless, **ESPECIALLY** if this notation is being used in other contexts/fields (Outside ML, Discrete Math, and Linear Algebra)\n",
        "* $\\in$ - pronounced as \"in\" - A symbol denoting that the value of a mentioned variable (usually $x$, $y$, etc.) exists within/is bounded by some set of numbers, meaning that it will _always_ be within that set and never be anything that's not part of it.\n",
        "* $\\{a, b, c\\}$ - pronounced \"set of ...\" - This is specificlly defined set (list) of numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol. (Note the curly braces $\\{ \\}$)\n",
        "* $\\mathbb{R}$ - pronounced \"\\<the set of> all real numbers\" - This symbol represents the set of **all** real numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol.\n",
        "* In this notebook, we also use the notation $\\mathbb{R}^d$ or $\\{a, b, c\\}^d$  , where $d$ indicates the dimensions/shape of whatever the variable we're referencing (Outside of this notebook, $\\mathbb{R}^n$ is often the same meaning where $d$ = $n$).\n",
        "    * e.g. $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ means that a ***matrix*** $\\hat{Y}$ in the shape of \"${4 \\times 1}$\", where each element of the matrix is a real number (part of the set $\\mathbb{R}$)\n",
        "    * e.g. $\\vec{x} \\in \\mathbb{R}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are all real numbers.\n",
        "    * e.g. $\\vec{x} \\in \\{0,1\\}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  \n",
        "    * e.g. $\\vec{\\hat{y}} \\in \\{0,1\\}$ is a ***vector*** $\\vec{\\hat{y}}$ in $1$ dimensional space, where the possible values of the vector's *component* is either $0$ or $1$\n",
        "    * e.g. $\\{0,1\\}^2$ is a ***vector*** (that's unnamed) in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZuLT_rXLtT4"
      },
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the process black-boxed by deep learning frameworks.\n",
        "\n",
        "\n",
        "## Example Task: Boolean Logic Gates\n",
        "In order to focus on explaining the internals of training, this notebook uses a simple and classic example: *boolean logic gates* (aka *threshold logic units*).\n",
        "\n",
        "When we talk about boolean logic, we refer to operations that exclusively use ***Truth Values***, which are *binary* values that can **only** be *True* or *False*, typically represented where $x=0$ means *False* and $x=1$ means *True*.\n",
        "\n",
        "\n",
        "*Boolean Logic Gates* take one or more boolean values as input and produce a single boolean output. Some basic *Boolean Logic Gates* are:\n",
        "* AND ($\\wedge$) - Takes two inputs (A & B) and outputs *True* **if and only if** both A and B are True. Otherwise outputs *False*.\n",
        "* OR ($\\vee$) - Takes two inputs (A & B) and outputs *True* **if either** A or B are *True*. Otherwise outputs *False*.\n",
        "* NOT ($\\lnot$) - Takes one input (A) and *inverts* it. if A is True, it outputs *False*. If A is False, it outputs *True*.\n",
        "* NAND ($\\uparrow$) - Takes two inputs (A & B) and outputs *True* **if only both outputs are the not true* A and B are not. Otherwise outputs *False*.\n",
        "    * This is the opposite of the AND Gate, effectivly as if you take the output of an AND gate and pass it through a NOT gate\n",
        "* NOR ($\\downarrow$) - Takes two inputs (A & B) and outputs *True* **if** A or B are *False*. Otherwise outputs *False*.\n",
        "    * This is the opposite of the NOR Gate, effectivly as if you take the output of an OR gate and pass it through a NOT gate\n",
        "* XOR ($\\oplus$) - *exclusive OR* - Takes two inputs (A & B) and outputs *True* if **only one input* (A or B, but not both at the same time) is *True*. Otherwise outputs *False*.\n",
        "* XNOR ($\\odot$) - *exclusive NOR* - Takes two inputs (A & B) and outputs *True* if **only both outputs are the same*. Otherwise outputs *False*.\n",
        "    * This is the opposite of the XOR Gate, effectivly as if you take the output of an XOR gate and pass it through a NOT gate\n",
        "<center><img src=\"img/logic_gates.png\" alt=\"Logic gate symbols, as typically used in electronics\"/></center>\n",
        "\n",
        "It's common to compile these operations into a \"truth table\", like below. Columns $A$ and $B$ represent the inputs, and the name of the *Boolean Logic Gates* represents the output for the given values of $A$ and $B$ in a row.\n",
        "| $A$ | $B$ | AND | OR | NOT* | NAND | NOR | XOR | XNOR |\n",
        "| :-: | :-: | :---: | :--: | :---: | :----: | :---: | :---: | :----: |\n",
        "| 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 0 |\n",
        "| 1 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
        "\n",
        "\\* Only uses one input, column A\n",
        "\n",
        "\n",
        "Defining $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and their inverted counterparts.\n",
        "\n",
        "Because they're only one layer, they're unable to represent logical compounds such as XOR/XNOR.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdFG8U2Net8"
      },
      "source": [
        "## Using numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug4zqMhLB-B6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5cM4JEsENr"
      },
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a **weight** (vector) ; $b \\in \\mathbb{R}$ is a **bias** (scalar) ; and $g(.)$ denotes an **activation function** (in this case, that's the Unit Step Function) (we assume $g(0)=0$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRpaNDA8BWJY"
      },
      "source": [
        "For simplicity, let us consider examples with two-dimensional inputs ($d=2$).\n",
        "We can represent an input vector $\\boldsymbol{x} \\in \\mathbb{R}^2$ and weight vector $\\boldsymbol{w} \\in \\mathbb{R}^2$ with `numpy.array`. We also define the bias term $b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyX1MfRvBD25"
      },
      "outputs": [],
      "source": [
        "# Define some weight, input, and bias values to use\n",
        "x = np.array([0, 1]) # 3 dimensional vector with possible values {0,1} - Inputs\n",
        "w = np.array([1.0, 1.0]) # 2 dimensional vector with possible values ℝ    - Weights\n",
        "b = 1.0 # scalar value                                                    - Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-P2AkbTCI0P"
      },
      "source": [
        "The following code computes $\\boldsymbol{w} \\cdot \\boldsymbol{x} + b$,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g_115nLCGqs"
      },
      "outputs": [],
      "source": [
        " np.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzhy8YFDIuf"
      },
      "source": [
        "We can apply the step function (also known as a Heaviside or Unit step function) as an *activation function*, $g()$:\n",
        "\n",
        "<img src=\"img/heaviside_step.png\" alt=\"Plot of the Heaviside Step Function\" style=\"background-color:white; width: 400px;\"/>\n",
        "When applied to the above result as $g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b)$, it yields a binary label, $\\hat{y}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV6a5JT5DH6c"
      },
      "outputs": [],
      "source": [
        "np.heaviside(np.dot(x, w) + b, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ppo_Jh8EEs3"
      },
      "source": [
        "#### Including the bias term into the weight vector\n",
        "\n",
        "For concise implementation, we include a bias term `b` as an additional dimension to the weight vector `w`. More concretely, we append an element with the value of $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x} = (0, 1) \\rightarrow \\boldsymbol{x}' = (0, 1, 1)\n",
        "$$\n",
        "and expand the dimension of the weight vector $\\boldsymbol{w} \\in \\mathbb{R}^{3}$.\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "In other words, $w_1$ and $w_2$ represent weights for $x_1$ and $x_2$, respectively, and $w_3$ is our bias value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmQGDVM2FnGy"
      },
      "outputs": [],
      "source": [
        "x = np.array([0, 1, 1])\n",
        "w = np.array([1.0, 1.0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53zn1dWQFvLw"
      },
      "source": [
        "We can simplify the code to predict a binary label $\\hat{y}$,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsgq_oOzF4PZ"
      },
      "outputs": [],
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SDTFENBA19"
      },
      "source": [
        "#### Training a NAND gate\n",
        "\n",
        "Let's train a NAND gate with two inputs. More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias value $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "| :---: |:---: | :---: |\n",
        "| 0 | 0 | 1 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "As explained earlier, we include the bias term into the last dimension.\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (50 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ygoUjQYrPoj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1., 1., 1., 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "lr = 0.5 #Learning Rate\n",
        "for t in range(50):\n",
        "    # Pick one random sample of traing data, at index (i), at random.\n",
        "    i = random.choice(range(len(y)))\n",
        "    # Predict the label for the instance x[i] with the current parameter w.\n",
        "    y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "    # Show the detail of the instance and the current parameter.\n",
        "    print(f'#{t:<2}: training sample index={i}, x={x[i]}, w={w}, y={y[i]}, y_pred={y_pred}, y_err={y[i] - y_pred}')\n",
        "    # Update the parameter.\n",
        "    loss = y[i] - y_pred\n",
        "    w += loss * lr * x[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8geTVVpnlu1O"
      },
      "source": [
        "We can confirm the learned parameter and classification results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYoeshu2rXdK"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOFgUFojraFA"
      },
      "outputs": [],
      "source": [
        "y_pred=np.heaviside(np.dot(x, w), 0)\n",
        "print(f\" Truth: {y}\")\n",
        "print(f\"  Pred: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl_ZAEguNzgU"
      },
      "source": [
        "### Single-layer perceptron with batching\n",
        "\n",
        "When training a model using a larger dataset with many samples, it's ideal to reduce the number of computations you perform - especially when running native python code (which is a lot slower than other languages). The common technique to speed up a machine-learning code written in Python is to run code using libraries that accelerate the large matrix operations, such as numpy (or Tensorflow, Keras, and Pytorch, but we'll get to that in a bit!)\n",
        "\n",
        "Even when using a libaray to accelerate these operations, we'll still run into issues with the sheer number of computations if we calculated the loss and updated the weights for every single image, so instead we use a technique called **batching**, where we calculate the loss of multiple images at a time and update the weights once every $n$ images, where $n$ is the number of images in one batch, or the **batch size**\n",
        "\n",
        "So, putting that into practice;\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "and if we give it 4 inputs, we'll get an output for each input\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix}\n",
        "  \\hat{y}_1 \\\\\n",
        "  \\hat{y}_2 \\\\\n",
        "  \\hat{y}_3 \\\\\n",
        "  \\hat{y}_4 \\\\\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix}\n",
        "  \\hat{y}_1 \\\\\n",
        "  \\hat{y}_2 \\\\\n",
        "  \\hat{y}_3 \\\\\n",
        "  \\hat{y}_4 \\\\\n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix}\n",
        "  \\boldsymbol{x}_1 \\\\\n",
        "  \\boldsymbol{x}_2 \\\\\n",
        "  \\boldsymbol{x}_3 \\\\\n",
        "  \\boldsymbol{x}_4 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD7f0rdz4wxS"
      },
      "outputs": [],
      "source": [
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "w = np.array([1.0, 0.5, -0.5])\n",
        "np.heaviside(np.dot(x, w), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfwL6Z8q5Cyu"
      },
      "source": [
        "The code below applies the Perceptron algorithm with batching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fK-_WimtPwb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "lr = 0.5\n",
        "for t in range(10):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0) # Instead of picking a single image to calculate with, we give it 4 at once\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    Yerr = (y - y_pred) #loss is now a 4 dimensional vector - one value for each input from x\n",
        "    print(loss)\n",
        "    w += lr * np.dot(Yerr, x) # update the weight parameters,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvR709WnUlH"
      },
      "source": [
        "We can confirm the learned parameters and classification results match what we expect/the example done without batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x4p1BldtQ-K"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-P2RpWrtVyf"
      },
      "outputs": [],
      "source": [
        "y_pred=np.heaviside(np.dot(x, w), 0)\n",
        "print(f\" Truth: {y}\")\n",
        "print(f\"  Pred: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxBcCSTtDvm"
      },
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch\n",
        "\n",
        "SGD works by updating the weights an iteration at a time using the function -\n",
        "$$ i=1, 2, ..., n $$\n",
        "$$ w_{n+1} = w_{n} - \\eta \\nabla Q_i(w)$$\n",
        "\n",
        "This is where $w$ is the weight, $\\eta$ is the rate of update (or the learning rate), and $\\nabla Q_i(w)$ is the gradient of a batch of data's loss.\n",
        "\n",
        "The weights are updated with each batch of loss, $Q_i(w)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bltpfNRctjV5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    w -= eta * np.dot((y_pred - y), x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bASDMfhtm-I"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-69r0c4KtqlW"
      },
      "outputs": [],
      "source": [
        "sigmoid(np.dot(x, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK6zfKzwXW73"
      },
      "source": [
        "## Challenge - Write a different loss function\n",
        "\n",
        "The above code updates the model for a specific loss function, $\\hat y - y$. Modify the code below to use a loss function that measures the absolute squared difference between the true and predicted valued!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrGQrhNXV3f"
      },
      "outputs": [],
      "source": [
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "\n",
        "    w -= \"??\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELUeNFRFuJv3"
      },
      "source": [
        "## Automatic differentiation\n",
        "\n",
        "#todo explain autodif anf usefulness\n",
        "\n",
        "\n",
        "Consider a loss function,\n",
        "$$\n",
        "l_{\\boldsymbol{x}}(\\boldsymbol{w}) = - \\log \\sigma(\\boldsymbol{w} \\cdot \\boldsymbol{x}) = - \\log \\frac{1}{1 + e^{-\\boldsymbol{w} \\cdot \\boldsymbol{x}}}\n",
        "$$\n",
        "\n",
        "This section shows implementations in different libraries of deep learning for computing the loss value $l_{\\boldsymbol{x}}(\\boldsymbol{w})$ and gradients $\\frac{\\partial l_{\\boldsymbol{x}}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$ when $\\boldsymbol{x} = (1, 1, 1)$ and $\\boldsymbol{w} = (1, 1, -1.5)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0DwVOyuXP_"
      },
      "source": [
        "### Challenge - Using autograd\n",
        "\n",
        "See [this link](https://github.com/HIPS/autograd) for a more in depth explaination of how autograd works.\n",
        "\n",
        "Write the above log loss function into the code, and watch the gradient change over time using the autograd function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyC9iOFO-1cd"
      },
      "outputs": [],
      "source": [
        "import autograd\n",
        "import autograd.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    #return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "    return \"\"\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = autograd.grad(loss)\n",
        "\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ_BH643pO9Y"
      },
      "source": [
        "### Using TensorFlow Eager\n",
        "\n",
        "See: https://www.tensorflow.org/guide/autodiff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp87EyeAp6CY",
        "outputId": "1d1cce00-cd1b-4e8f-d5f6-a4b8e684b7a3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dtype = tf.float32\n",
        "\n",
        "x = tf.constant([1, 1, 1], dtype=dtype, name='x')\n",
        "w = tf.Variable([1.0, 1.0, -1.5], dtype=dtype, name='w')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    loss = -tf.math.log(tf.math.sigmoid(tf.tensordot(x, w, 1)))\n",
        "\n",
        "print(loss.numpy())\n",
        "print(tape.gradient(loss, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGS23bDSazMJ"
      },
      "source": [
        "### Single-layer neural network with high-level NN modules (w/ optimizers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q-zWy4TLtUB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2, l1\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX6K-CT3LtUC"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
        "y = tf.Variable([[1], [1], [1], [0]], dtype=tf.float32)\n",
        "\n",
        "eta = 0.5\n",
        "\n",
        "# Define that the input is 2 dimensions long\n",
        "input_layer = Input(shape=(2,))\n",
        "\n",
        "# Define the output is a single dimension\n",
        "dense_layer = Dense(1)(input_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=dense_layer)\n",
        "\n",
        "loss_function = BinaryCrossentropy()\n",
        "loss_history = []\n",
        "\n",
        "for t in range(10):                    # Iterate over 100 batches of data\n",
        "    trainable_variables = model.trainable_variables\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "\n",
        "        y_pred = model(x)                   # Make predictions.\n",
        "        loss = loss_function(y_pred, y)     # Compute the loss.\n",
        "        loss_history.append(loss)           # Record the loss value.\n",
        "\n",
        "        # Calculate gradients with respect to every trainable variable\n",
        "        grad = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "        for param_number, gradient in enumerate(grad):\n",
        "            model.trainable_variables[param_number].assign_sub(eta * gradient)   # Update the parameters using SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwXFLW6MN8AD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(loss_history)),loss_history)\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sABi6XbOLXE"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgs8e4d2LtUC"
      },
      "source": [
        "## The Alt Method - Challenge !\n",
        "\n",
        "Keras can hide the training loops by specifying the loss and optimizer to the `compile()` method.\n",
        "\n",
        "Use this version to train a model and plot the history.\n",
        "The syntatx for the training loop is [found here](https://keras.io/api/models/model_training_apis/#fit-method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZyYVRSCLtUC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define that the input is 2 dimensions long\n",
        "input_layer = \"?\"\n",
        "\n",
        "# Define the output is a single dimension\n",
        "dense_layer = \"?\"\n",
        "\n",
        "model = Model(input_layer, dense_layer)\n",
        "model.compile(optimizer=\"SDG\", loss=\"mse\") # Use the automatic training loop to update the parameters\n",
        "\n",
        "\n",
        "history = model.fit(\"?????\").history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D20KVsRJ5wJK"
      },
      "outputs": [],
      "source": [
        "loss_history = history[\"??\"]\n",
        "plt.plot(range(len(loss_history)),loss_history)\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfuoJMeqbClA"
      },
      "source": [
        "# Multi-layer neural network with high-level NN modules\n",
        "\n",
        "introduce the idea of layers in a network\n",
        "\n",
        "do Jet Tagger stuff\n",
        "\n",
        "based on [the hls4ml tutorial part 1](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part1_getting_started.ipynb)\n",
        "\n",
        "TODO: Some of this is based on older sklearn/tf and needs to be updated to work properly...\n",
        "\n",
        "\n",
        "## Particle Physics Example: Jet Tagging\n",
        "\n",
        "<img src=\"img/jet_tagger_jets.png\" alt=\"2D Representations of the different kinds of particle jets the neural network will classify\" style=\"background-color:white; width: 800px;\"/>\n",
        "\n",
        "blah\n",
        "\n",
        "### Multi-Layer Perceptron - Your first Deep Neural Network!\n",
        "\n",
        "<img src=\"img/jet_tagger_mlp.png\" alt=\"Graph of the Jet Tagger MLP Neural Network\" style=\"background-color:white; width: 400px;\"/>\n",
        "\n",
        "blah\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU2G2uffLtUD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxsTZfzXLtUE"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "data = jet_tagging_data\n",
        "X, y = data['data'], data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfP4G9WNLtUE",
        "outputId": "677c6cd8-aaa1-44b4-d38f-5d03a8e5405c"
      },
      "outputs": [],
      "source": [
        "# look at some example data\n",
        "print(data['feature_names'])\n",
        "print(X.shape, y.shape)\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8i_17fRLtUE"
      },
      "source": [
        "## Data pre-processing\n",
        "As it stands, our data isn't quite ready to feed into a network. We need to do a little bit of work ahead of time (**preprocessing**) to format the data and apply some statistical methods to make things easier for the network to understand.\n",
        "\n",
        "### Scaling Data\n",
        "\n",
        "When training a model, the model needs to understand what things mean physically. Unfortunately, all they model knows is numbers. Because of this, a variable that has a very large scale will get undue importance according to the model. Because of this, we can scale the input to a range that is consistent between all the variables in the model.\n",
        "In this scale, we'll do it by scaling according to the mean and standard deviation of the variable; using a `StandardScaler`\n",
        "\n",
        "The standard scaler transforms variables according to the formula:\n",
        "\n",
        "$$ X_{scaled} = \\frac{(X_{original} - \\mu)}{ \\sigma }$$\n",
        "\n",
        "### One-Hot Encoding\n",
        "\n",
        "Models cannot natively understand strings (words, letters, etc), only numbers. Because of this, we need to give them a little bit of help by encoding labels. When labels do not have any sort of order, we transform them using one-hot encoding.\n",
        "\n",
        "A set of labels ` labels = ['a', 'b', 'c']` will be transformed into the binary logic -\n",
        "\n",
        "| | $a$ | $b$ | $c$ |\n",
        "| :---: | :---: |:---: | :---: |\n",
        "|1 | 1 | 0 | 0 |\n",
        "|2 | 0 | 1 | 0 |\n",
        "|3 | 0 | 0 | 1 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTtc9DaTLtUE",
        "outputId": "dc503a72-ec67-4ab2-d5af-d5daaf32871e"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y, 5)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBs24vTLLtUG"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_val = scaler.fit_transform(X_train_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1sDiq8DLtUG"
      },
      "source": [
        "## Writing our MLP in Keras\n",
        "\n",
        "We'll define our MLP using a `functional` api. This means we'll define the model layer by layer, and connect them together.\n",
        "\n",
        "The model is built out of 3 dense layers with an activation between each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjHOzZcXLtUG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0M-BqfKLtUG"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_layer = Input(shape=(16,), name='input')\n",
        "x = Dense(64,name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(input_layer)\n",
        "x = ReLU()(x)\n",
        "x = Dense(32,name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "output = Softmax()(x)\n",
        "\n",
        "model = Model(input_layer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRk3SAhJVOIp"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIzFc1MRLtUG"
      },
      "source": [
        "## Training our MLP\n",
        "\n",
        "To train the model, we define the training optimizer (how the weights will be updated, instead of using SDG, we can pick anything), the loss being used (this is a classifier model, so we'll use a classifier loss), and the data being used to train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6ss25zA9nPk"
      },
      "outputs": [],
      "source": [
        "# Compile our model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAdWR6o-LtUG"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "history = model.fit(\n",
        "    X_train_val, # Input data\n",
        "    y_train_val, # Truth Output data\n",
        "    batch_size=1024, # Batch Size\n",
        "    epochs=30, # How many times will we iterate over the whole training dataset to train the model?\n",
        "    validation_split=0.25, # How much of the train dataset do we want to reserve as our validation split?\n",
        "    shuffle=True).history #Do we want to to order of our training samples to be shuffled?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "-_6Hle_e6IXM",
        "outputId": "61ea3eac-efcc-422f-9f0e-c8f887142248"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_history = history['loss']\n",
        "epochs = range(len(loss_history))\n",
        "\n",
        "plt.plot(epochs, loss_history)\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLFGZWL0--2n"
      },
      "outputs": [],
      "source": [
        "## Check model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti0_Pi56LtUH"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.metrics import auc, roc_curve, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    # plt.title(title)\n",
        "    cbar = plt.colorbar()\n",
        "    plt.clim(0, 1)\n",
        "    cbar.set_label(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def plotRoc(fpr, tpr, auc, labels, linestyle, legend=True):\n",
        "    for _i, label in enumerate(labels):\n",
        "        plt.plot(\n",
        "            tpr[label],\n",
        "            fpr[label],\n",
        "            label='{} tagger, AUC = {:.1f}%'.format(label.replace('j_', ''), auc[label] * 100.0),\n",
        "            linestyle=linestyle,\n",
        "        )\n",
        "    plt.semilogy()\n",
        "    plt.xlabel(\"Signal Efficiency\")\n",
        "    plt.ylabel(\"Background Efficiency\")\n",
        "    plt.ylim(0.001, 1)\n",
        "    plt.grid(True)\n",
        "    if legend:\n",
        "        plt.legend(loc='upper left')\n",
        "    plt.figtext(0.25, 0.90, 'hls4ml', fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
        "\n",
        "\n",
        "def rocData(y, predict_test, labels):\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    auc1 = {}\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        df[label] = y[:, i]\n",
        "        df[label + '_pred'] = predict_test[:, i]\n",
        "\n",
        "        fpr[label], tpr[label], threshold = roc_curve(df[label], df[label + '_pred'])\n",
        "\n",
        "        auc1[label] = auc(fpr[label], tpr[label])\n",
        "    return fpr, tpr, auc1\n",
        "\n",
        "\n",
        "def makeRoc(y, predict_test, labels, linestyle='-', legend=True):\n",
        "    if 'j_index' in labels:\n",
        "        labels.remove('j_index')\n",
        "\n",
        "    fpr, tpr, auc1 = rocData(y, predict_test, labels)\n",
        "    plotRoc(fpr, tpr, auc1, labels, linestyle, legend=legend)\n",
        "    return predict_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "1BF6-W_J-82M",
        "outputId": "39415e04-0044-48f4-c1f1-b220e8496e8d"
      },
      "outputs": [],
      "source": [
        "y_keras = model.predict(X_test)\n",
        "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
        "plt.figure(figsize=(9, 9))\n",
        "_ = makeRoc(y_test, y_keras, le.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3K-JHWLtUH"
      },
      "source": [
        "## Attribution, Sources, and Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4kLg0dTLtUH"
      },
      "source": [
        "This notebook was derived from two, seperate notebooks from the Tokyo Institute of Technology's ART.T458: Advanced Machine Learning course, and were originally authored by Prof. Naoaki Okazaki.\n",
        "\n",
        "The original notebooks (and accompanying material) can be found here: https://chokkan.github.io/deeplearning/\n",
        "\n",
        "Modifications to this notebook were done by Ben Hawks for the 2023 Fermilab and Brookhaven National Lab Summer Exchange School\n",
        "\n",
        "Various sources for images and other materials used is listed below:\n",
        "1. [Matrix Transpose Gif by Lucas Vieira via Wikipedia](https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif)\n",
        "3. [Boolean Logic Gates Image/Symbols (Digilent) ](https://digilent.com/blog/logic-gates/)\n",
        "    * Original/Individual Symbols used (IEEE Std 91/91a-1991 \"Distinctive Shapes\") are originally via [Inductiveload via Wikipedia](https://en.wikipedia.org/wiki/Logic_gate#Symbols)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
