{"cells":[{"cell_type":"markdown","metadata":{"id":"d2ad3bde"},"source":["# Classifying high-redshift merging galaxies with deep neural networks and DeepMerge\n","\n","[SLIDES FROM THIS MORNING](https://docs.google.com/presentation/d/1GUPf2pCBxNch-KI--KSnxFP8LmAiqSc0HZLOeavULQw/edit?usp=sharing)\n"],"id":"d2ad3bde"},{"cell_type":"markdown","metadata":{"id":"3a7f9875"},"source":["***"],"id":"3a7f9875"},{"cell_type":"markdown","metadata":{"id":"ebe2a9d0"},"source":["## Learning Goals\n","\n","\n","**In this tutorial, you will see an example of building, compiling, and training a CNN on simulated astronomical data.**  By the end of this tutorial you will have a working example of a simple Convolutional Neural Network (CNN) in Keras."],"id":"ebe2a9d0"},{"cell_type":"markdown","metadata":{"id":"324aa021"},"source":["## Introduction\n","CNNs are a class of machine learning (ML) algorithms that can extract information from images.  In this notebook, you will walk through the basic steps of applying a CNN to data:\n","1. Load the data and visualize a sample of the data\n","2. Divide the data into training, validation, and testing sets.\n","3. Build a CNN in Keras\n","4. Compile the CNN\n","5. Train the CNN to perform a classification task\n","6. Evaluate the results.\n","\n","CNNs can be applied to a wide range of image recognition tasks, including classification and regression.  Here, we will build, compile, and train CNN to classify whether a galaxy has undergone a merger from simulated Hubble Space Telescope images of galaxies. This work is based on the public data and code from <a href='https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract'>DeepMerge (Ciprijanovic et al. 2020)</a>.\n","\n","**NOTE:** *The [DeepMerge team has publicly-available code](https://github.com/deepskies/deepmerge-public) for demonstrating the architecture and optimal performace of the model, which we encourage you to check out! The goal of this notebook is to step through the model building and training process.*\n"],"id":"324aa021"},{"cell_type":"markdown","metadata":{"id":"o-5gBCIHY8uK"},"source":["## Artificial Neurons - building blocks of neural networks\n","\n","Neural networks learn from examples just like humas! Their building blocks are artificial neurons that are inspired by acctuall neurons in our brains (the idea was firs born in 1943. , Warren McCulloch and Walter Pitts).\n","\n","![](https://historyofinformation.com/images/Screen_Shot_2020-09-09_at_6.46.46_AM_big.png)\n","\n","Actual neuron:\n","\n","![](https://www.jeremyjordan.me/content/images/2018/01/Neuron_Hand-tuned.png)\n","\n","Artificial neuron:\n","\n","![](https://www.jeremyjordan.me/content/images/2018/01/single_neuron.jpg)"],"id":"o-5gBCIHY8uK"},{"cell_type":"markdown","metadata":{"id":"76ef82ab"},"source":["## Convolutional Neural Network (CNN)  for Image Classification"],"id":"76ef82ab"},{"cell_type":"markdown","metadata":{"id":"plXBFEgvT7Lw"},"source":["CNNs are comprised from two types of layers: **2D convolutional layers** and **1D fully-connected (or dense) layers**.\n","\n","In each convolutional layer we scan the images with filters that try to search out some combination of color and morphology -- like edges, wavelet patterns, etc. The next layer of the CNN then takes the previous layer's output as input, and then search for higher-level (or more complex) morphological features. For example, multiple edges at different angles can be used to detect curves, and different kinds of wavelets might be combined to identify textured patterns.\n","\n","In functional form, a neural network might look something like this:\n","\n","$$ {\\rm CNN}(x) = {\\rm Layer_{\\rm final}} \\Big [ \\cdots \\Big ( {\\rm Layer_2} \\big ( {\\rm Layer}_1(x) \\big ) \\Big) \\Big ], $$\n","\n","where each layer might contain a convolutional layer and other ingredients (see section below).\n","\n","![Credit: https://www.analyticsvidhya.com/blog/2020/10/what-is-the-convolutional-neural-network-architecture/](https://editor.analyticsvidhya.com/uploads/90650dnn2.jpeg)\n","\n","\n","How do convolutions work? These are just matrix multiplications!\n","<!-- ![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif) -->\n","![](http://media5.datahacker.rs/2020/12/movie1-1-1.gif)\n","\n","After enough seful features has been extracted and the dimensionality of the layer has been reduced this information is flattened into a 1D vector, and used in subsequent dense layers to learn to perform a specific task, such as image classification!\n","\n","Additional imporant terms:\n","\n","**Batch normalization** - Due to memory constraints, we need to feed in images one \"batch\" at a time. Images flow through the network one batch at a time, so the inputs should be arrays of size  (ùëÅbatch size,ùëÅfilters,ùëÅheight,ùëÅwidth) . For example a batch of 64 three-color  100√ó100 images has the array shape  (64,3,160,160) .\n","\n","Batch normalization (batchnorm) is a mechanism that keeps the distribution of activations close to a Gaussian distribution to allow the model to train better.\n","\n","**Pooling** - As we go deeper into the network the number of filter increases! To lessen the computational burden, we rely on pooling layers: often max pooling or mean pooling are commonly seen. For example, if you double the number of filters, but halve the height and width of the output image by using max pooling with a  2√ó2  kernel, then you still have fewer outputs than from the preceding layer!\n","\n","**Activation funcitons** - they add nonlinearities to the network allowing us to combine huge number of neurons and build comlex models. The rules of linear algebra state that the product of any number of matrices can be represented by yet another matrix. If you want something that's not represented by just a single matrix, you need a non-linear function in between neural network layers. Some of the most commontly used activation functions are ReLU, Softmax, Sigmoid etc.\n","\n","![](https://media.licdn.com/dms/image/C4E12AQFqIFCj71YJPw/article-cover_image-shrink_600_2000/0/1620764635917?e=2147483647&v=beta&t=E6iCHNUyncJu9QUjCX4EmVLgQanUw_WS6KnWrlV4Roc)"],"id":"plXBFEgvT7Lw"},{"cell_type":"markdown","metadata":{"id":"618ee480"},"source":["## Imports\n","This notebook uses the following packages:\n","- *numpy* to handle array functions\n","- *astropy* for downloading and accessing FITS files\n","- *matplotlib.pyplot* for plotting data\n","- *keras* and *tensorflow* for building the CNN"],"id":"618ee480"},{"cell_type":"code","execution_count":null,"metadata":{"id":"519891d4"},"outputs":[],"source":["# arrays\n","import numpy as np\n","\n","# fits\n","from astropy.io import fits\n","from astropy.utils.data import download_file\n","from astropy.visualization import simple_norm\n","\n","# plotting\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","# keras\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Flatten, Dense, Activation, Dropout, BatchNormalization\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D\n","from keras.regularizers import l2\n","from keras.callbacks import EarlyStopping\n","\n","# sklearn (for machine learning)\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score"],"id":"519891d4"},{"cell_type":"markdown","metadata":{"id":"62a3fcb1"},"source":["### 1. Load the data and visualize a sample of the data\n","\n","Load the simulated galaxy observations (3-band images) and merger probabilities (output labels).  These data are described in detail in  <a href='https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract'>Ciprijanovic et al. 2020</a>.\n","\n","In total, there are 15,426 simulated images, each in three filters (F814W from the Advanced Camera for Surveys and F160W from the Wide Field Camera 3 on the Hubble Space Telescope, and F), retrieved and augmented from synthetic observations of the Illustris cosmological simulation. The sample includes 8120 galaxy mergers and 7306 non-mergers. Two versions of the sample are available, with and without realistic observational and experimental noise (\"pristine\" and \"noisy\").\n","\n","These datasets are hosted at the Mikulski Archive for Space Telescopes as an HLSP. See the [DEEPMERGE](https://stdatu.stsci.edu/hlsp/deepmerge) website for more information.\n","\n","The CNN will be trained to distinguish between merging and non-merging galaxies.\n"],"id":"62a3fcb1"},{"cell_type":"markdown","metadata":{"id":"c8a2527c"},"source":["### Load the data"],"id":"c8a2527c"},{"cell_type":"markdown","metadata":{"id":"9944bd61"},"source":["The simulated images are stored in FITS format. We refer you to the [Astropy Documentation](https://docs.astropy.org/en/stable/io/fits/index.html) for further information about this format."],"id":"9944bd61"},{"cell_type":"markdown","metadata":{"id":"182a55ee"},"source":["For this example, we will download the \"pristine\" set of galaxy images, i.e., those without added observational noise. To select the \"noisy\" sample, change the version below. Alternatively, you can download data files from the [DEEPMERGE](https://stdatu.stsci.edu/hlsp/deepmerge) website."],"id":"182a55ee"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c01626c8"},"outputs":[],"source":["version = 'pristine'"],"id":"c01626c8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"04f9869c"},"outputs":[],"source":["file_url = 'https://archive.stsci.edu/hlsps/deepmerge/hlsp_deepmerge_hst-jwst_acs-wfc3-nircam_illustris-z2_f814w-f160w-f356w_v1_sim-'+version+'.fits'\n","hdu = fits.open(download_file(file_url, cache=True, show_progress=True))"],"id":"04f9869c"},{"cell_type":"markdown","metadata":{"id":"18e3a0dd"},"source":["Explore the header of the file for information about its contents"],"id":"18e3a0dd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2753e2fa"},"outputs":[],"source":["hdu[1].header"],"id":"2753e2fa"},{"cell_type":"markdown","metadata":{"id":"cc589f00"},"source":["The file includes a primary header card with overall information, an image card with the simulated images, and a bintable with the merger labels for the images (1=merger, 0=non-merger)."],"id":"cc589f00"},{"cell_type":"markdown","metadata":{"id":"aeaef9ba"},"source":["### Plot example images\n","\n","For a random selection of images, plot the images and their corresponding labels:"],"id":"aeaef9ba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e79adba"},"outputs":[],"source":["# select random image indices:\n","# Task: Create a NumPy array consisting of 16 random numbers ranging from 0 to len(hdu[1].data)\n","# Hint: Refer to the NumPy documentation to complete the line below\n","\n","\n","example_ids = ????\n","\n","# pull the F160W image (index=1) from the simulated dataset for these selections\n","\n","examples = [hdu[0].data[j,1,:,:] for j in example_ids]\n","\n","# initialize your figure\n","fig1=plt.figure(figsize=(8,8))\n","\n","# loop through the randomly selected images and plot with labels\n","for i, image in enumerate(examples):\n","    plt.subplot(4, 4, i + 1)\n","    plt.axis(\"off\")\n","\n","    norm = simple_norm(image, 'log')\n","\n","    plt.imshow(image, aspect='auto', cmap='viridis', norm=norm)\n","    plt.title('Merger='+str(bool(hdu[1].data[example_ids[i]][0])))\n","\n","plt.show()"],"id":"2e79adba"},{"cell_type":"markdown","metadata":{"id":"408afadf"},"source":["## 2. Divide data into training, validation, and testing sets"],"id":"408afadf"},{"cell_type":"markdown","metadata":{"id":"857ab807"},"source":["To divide the data set into training, validation, and testing data we will use Scikit-Learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n","\n","![](https://www.brainstobytes.com/content/images/2020/01/Sets.png)\n","\n","We will denote the input images as \"X\" and the output labels as \"Y\"."],"id":"857ab807"},{"cell_type":"code","execution_count":null,"metadata":{"id":"40cef745"},"outputs":[],"source":["X = hdu[0].data\n","y = hdu[1].data"],"id":"40cef745"},{"cell_type":"markdown","metadata":{"id":"c578392b"},"source":["Following the authors, we will split the data into 70:10:20 ratio of train:validate:test\n"],"id":"c578392b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"65eef99f"},"outputs":[],"source":["X = np.asarray(X).astype('float32')\n","y = np.asarray(y).astype('float32')\n","\n","# Task: Split the data into training and validation sets\n","# Hint: Look for the function train_test_split(???) in scikit-learn\n","\n","# First split off 30% of the data for validation+testing\n","X_train, X_split, y_train, y_split = ???\n","\n","# Divide this subset into validation and testing sets in 1:2 ratio\n","X_val, X_test, y_val, y_test = ???\n"],"id":"65eef99f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"TT-X5-qsx4Pw"},"outputs":[],"source":["# Check that the length of each dataset is correct\n","# Task: Complete the line below with the corresponding variable name\n","\n","\n","print(f\"The length of the full image-dataset is: {??}\"\n","print(f\"The length of the split image-dataset is: {??}\"\n","print(f\"The length of the validation image-dataset is: {??}\"\n","print(f\"The length of the test image-dataset is: {??}\"\n","print(\"\")\n","print(f\"The length of the full label-dataset is: {??}\"\n","print(f\"The length of the split label-dataset is: {??}\"\n","print(f\"The length of the validation label-dataset is: {??}\"\n","print(f\"The length of the test label-dataset is: {??}\"\n","\n","# Is the data divided in the correct ratios?"],"id":"TT-X5-qsx4Pw"},{"cell_type":"markdown","metadata":{"id":"8a769e47"},"source":["Next, reshape the image array as follows:  (number_of_images, image_width, image_length, 2)\n","This is referred to as a \"channels last\" approach, where the final axis denotes the number of \"colors\" or \"channels\".  Our two-filter images have two channels, while RGB images have three.  CNN's will work with an arbitrary number of channels."],"id":"8a769e47"},{"cell_type":"code","execution_count":null,"metadata":{"id":"19f0cf02"},"outputs":[],"source":["imsize = np.shape(X_train)[2]\n","\n","X_train = X_train.reshape(-1, imsize, imsize, 3)\n","X_valid = X_val.reshape(-1, imsize, imsize, 3)\n","X_test = X_test.reshape(-1, imsize, imsize, 3)"],"id":"19f0cf02"},{"cell_type":"markdown","metadata":{"id":"fe1a5163"},"source":["### 3. Build a CNN in Keras\n","\n","Here, we will build the model described in Section 3 of [Ciprijanovic et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract)\n","\n","Further details about Conv2D, MaxPooling2D, BatchNormalization, Dropout, and Dense layers can be found in the [Keras Layers Documentation](https://keras.io/api/layers/).  Further details about the sigmoid and softmax activation function can be found in the [Keras Activation Function Documentation](https://keras.io/api/layers/activations/)."],"id":"fe1a5163"},{"cell_type":"code","execution_count":null,"metadata":{"id":"580ca93f"},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# generate the model architecture\n","# Written for Keras 2\n","# ------------------------------------------------------------------------------\n","\n","#Visual, interactive (in-out)\n","\n","# Define architecture for model\n","data_shape = np.shape(X)\n","input_shape = (imsize, imsize, 3)\n","\n","x_in = Input(shape=input_shape)\n","c0 = Convolution2D(8, (5, 5), activation='relu', strides=(1, 1), padding='same')(x_in)\n","b0 = BatchNormalization()(c0)\n","d0 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b0)\n","e0 = Dropout(0.5)(d0)\n","\n","c1 = Convolution2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same')(e0)\n","b1 = BatchNormalization()(c1)\n","d1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b1)\n","e1 = Dropout(0.5)(d1)\n","\n","c2 = Convolution2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same')(e1)\n","b2 = BatchNormalization()(c2)\n","d2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b2)\n","e2 = Dropout(0.5)(d2)\n","\n","f = Flatten()(e2)\n","z0 = Dense(64, activation='softmax', kernel_regularizer=l2(0.0001))(f)\n","z1 = Dense(32, activation='softmax', kernel_regularizer=l2(0.0001))(z0)\n","y_out = Dense(1, activation='sigmoid')(z1)\n","\n","\n","# Task: Write the inputs and outputs defined above to build the model\n","\n","model = Model(inputs=???, outputs=???)"],"id":"580ca93f"},{"cell_type":"markdown","metadata":{"id":"2454f58f"},"source":["### 4. Compile the CNN\n","\n","Next, we compile the model.  As in [Ciprijanovic et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract), we select the Adam opmimizer and the binary cross entropy loss function (as this is a binary classification problem).\n","\n","You can learn more about [optimizers](https://keras.io/api/optimizers/) and more about [loss functions for regression tasks](https://keras.io/api/losses/) in the [Keras documentation](https://keras.io/)"],"id":"2454f58f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b90b61ba"},"outputs":[],"source":["# Compile Model\n","\n","optimizer = 'adam'\n","metrics = ['accuracy']\n","loss = 'binary_crossentropy'\n","model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","model.summary()"],"id":"b90b61ba"},{"cell_type":"markdown","metadata":{"id":"rgsWMJ_Nx-Sj"},"source":["## Loss function\n","\n","During training we need to choose a function that describes how well the model is doing - we call this a loss function. The goal of the trainign proceadure is to update the weights of the model such that over time the loss function is minimized. The most common loss function for classification tasks is **cross-entropy**.\n","![](https://miro.medium.com/v2/resize:fit:919/1*ETtY7KCrzAlOmLeyDWE4Xg.png)\n","\n","In case of 2 classes we can write it as (and call it binarry cross-entropy):\n","![](https://i.stack.imgur.com/HlYNr.png)\n","\n","Loss landscape:\n","\n","![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/63d287c3b82b3b3dffedb9b5_Loss%20landscape%20during%20model%20optimization.webp)\n","\n","**Gradient Descent**\n","\n","The gradient,  $\\nabla_\\theta J(\\theta; x)$ , points in the direction (in our model parameter space) that corresponds to lower loss - finds better values of our model parameters  $\\theta$ in order to improve predictions.\n","\n","The smoothness of our loss landscape matters a lot here, since the direction of steepest descent might vary wildly. This is a challenging problem for optmization, which is why it is common to use adaptive approaches, such as **Adam aptimizer** (it keeps track of previous update directions, and includes a small contribution from those prior steps in the update rule).\n","\n","![](https://miro.medium.com/v2/resize:fit:1200/1*STiRp7PW5yIrvYZupZA6nw.gif)\n","\n","## Learning rate\n","\n","\n","The learning rate is the most important hyperparameter for *efficiently* training a neural network. It determines how large each update is. For example, a 1-dimension loss landscape $J(\\theta)$ is shown below, while gradients multiplied by the learning rate are shown as red arrows:\n","\n","![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"],"id":"rgsWMJ_Nx-Sj"},{"cell_type":"markdown","metadata":{"id":"81fca68b"},"source":["### 5. Train the CNN to perform a classification task\n","\n","We will start with training for 20 epochs, but this almost certainly won't be long enough to get great results.  Once you've run your model and evaluated the fit, you can come back here and run the next cell again for 100 epochs or longer.  \n","\n","You can learn more about model.fit [here](https://keras.rstudio.com/reference/fit.html)"],"id":"81fca68b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7b0bf55"},"outputs":[],"source":["nb_epoch = 5\n","batch_size = 128\n","shuffle = True\n","\n","# Train\n","history = model.fit(X_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=nb_epoch,\n","                    validation_data=(X_valid, y_val),\n","                    shuffle=shuffle,\n","                    verbose=True)\n","\n","# Task (optional): Change the number of epochs to 10 or more! (one epoch is around a minute!)"],"id":"b7b0bf55"},{"cell_type":"markdown","metadata":{"id":"7f92fad3"},"source":["### 6. Visualize CNN performance\n","\n","To visualize the performance of the CNN, we plot the evolution of the accuracy and loss as a function of training epochs, for the training set and for the validation set."],"id":"7f92fad3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae194501"},"outputs":[],"source":["# plotting from history\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","epochs = list(range(len(loss)))\n","figsize=(6,4)\n","fig, axis1 = plt.subplots(figsize=figsize)\n","plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n","plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"validation accuracy\")\n","\n","plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n","plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"validation loss\")\n","\n","\n","plots = plot1_loss + plot1_val_loss\n","labs = [l.get_label() for l in plots]\n","axis1.set_xlabel('Epoch')\n","axis1.set_ylabel('Loss/Accuracy')\n","plt.title(\"Loss/Accuracy History (Pristine Images)\")\n","plt.tight_layout()\n","axis1.legend(loc='lower right')"],"id":"ae194501"},{"cell_type":"markdown","metadata":{"id":"IsHeBto-J1QU"},"source":["So what is validation set acctually used for?\n","\n","Neural networks are very complex - they can easily overfit to the data. We should always check if the model is able to perform on new unseen data, adn sto the training before performance on the new data starts to drop (signs of overfitting). We use validation set after every epoh to check if the model is overfitting or not.\n","\n","![](https://miro.medium.com/v2/resize:fit:1400/1*JZbxrdzabrT33Yl-LrmShw.png)\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/Screenshot-2020-02-06-at-11.09.13.png)"],"id":"IsHeBto-J1QU"},{"cell_type":"markdown","metadata":{"id":"3RZSZRc1UuEO"},"source":["### 6. Presenting your results\n","\n","To truly test the performance we need to show how well the model classifies\n","unseen images (test set).\n","\n","First let's apply the train model to the test data (evaluate the model).\n","You can learn more about model.evaluate [here](https://keras.rstudio.com/reference/evaluate.keras.engine.training.Model.html)."],"id":"3RZSZRc1UuEO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ng6XoZ2PVhVU"},"outputs":[],"source":["# Evaluate model\n","\n","# Task: Complete the line below to get the score\n","# Hint: Look for the model.evaluate function in the scikit-learn documentation\n","# Hint2: Use the test data\n","\n","score = model.evaluate(???, ???, verbose=True)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"],"id":"Ng6XoZ2PVhVU"},{"cell_type":"markdown","metadata":{"id":"uF5TfL5AakTj"},"source":["Lets generate predictions and see ow many images were correctly classified.\n","You can read more about model. predicts [here](https://keras.rstudio.com/reference/predict.keras.engine.training.Model.html).\n","\n","Since our model outputs a single number between 0-1 we need to pick a treshold that divides the two classes. Let's pick 0.5 for now."],"id":"uF5TfL5AakTj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynAESWioUrk4"},"outputs":[],"source":["# predict\n","\n","prob = model.predict(X_test)\n","pred =  (prob > 0.5).astype('int32')\n","\n","# measure confusion\n","labels=[0, 1]\n","cm = confusion_matrix(y_test, pred, labels=labels)\n","cm = cm.astype('float') # regular CM\n","cm_norm = cm / cm.sum(axis=1)[:, np.newaxis] # normalized CM\n","\n","\n","#plotting\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(cm)\n","plt.title('Confusion matrix', y=1.08)\n","fig.colorbar(cax)\n","ax.set_xticklabels([''] + labels)\n","ax.set_yticklabels([''] + labels)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","fmt = '.0f'\n","thresh = cm.max() / 2.\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        ax.text(j, i, format(cm[i, j], fmt),\n","        ha=\"center\", va=\"center\",\n","        color=\"white\" if cm[i, j] < thresh else \"black\")\n","plt.show()"],"id":"ynAESWioUrk4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"256doe0cbz-1"},"outputs":[],"source":["#plotting the normalized version\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(cm)\n","plt.title('Normalized confusion matrix )', y=1.08)\n","fig.colorbar(cax)\n","ax.set_xticklabels([''] + labels)\n","ax.set_yticklabels([''] + labels)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","fmt = '.2f'\n","thresh = cm_norm.max() / 2.\n","for i in range(cm_norm.shape[0]):\n","    for j in range(cm_norm.shape[1]):\n","        ax.text(j, i, format(cm_norm[i, j], fmt),\n","        ha=\"center\", va=\"center\",\n","        color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n","plt.show()"],"id":"256doe0cbz-1"},{"cell_type":"markdown","metadata":{"id":"Oe4wd6Ubx4P3"},"source":["### Confusion Matrix"],"id":"Oe4wd6Ubx4P3"},{"cell_type":"markdown","metadata":{"id":"U8mu5x4sx4P3"},"source":["Another option for data visualization is the Confusion Matrix.\n","\n","It showa the counts of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n","\n","A perfect classifier would have all predictions along the diagonal (top-left to bottom-right), indicating correct classifications. However, confusion matrices for real-world classifiers may have varied distributions across these categories leading to off-diagonal values.\n","\n","Understanding the confusion matrix helps in assessing the model's strengths and weaknesses, particularly in identifying where it tends to misclassify instances."],"id":"U8mu5x4sx4P3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_RfNzx3x4P3"},"outputs":[],"source":["# Calculate confusion matrix\n","\n","# Task: Complete the line below with the true labels and predicted labels of the model\n","\n","confusion_matrix = confusion_matrix(???, ???)\n","\n","# Visualize the matrix\n","\n","print(confusion_matrix)\n","\n","# Plot confusion matrix\n","\n","plt.figure(figsize=(5, 5))\n","sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()"],"id":"U_RfNzx3x4P3"},{"cell_type":"markdown","metadata":{"id":"t2uXN-CXx4P4"},"source":["### ROC courve"],"id":"t2uXN-CXx4P4"},{"cell_type":"markdown","metadata":{"id":"gzZsHwSDclhN"},"source":["Another common plot ir the Receiver Operating Characteristic (ROC) plot.\n","\n","It shows the trade-off between TP and FP.\n","\n","A perfect cassifier has a curve that approaches top left corner, while a classifier that randomly predicts labels has ROC curve close to the diagonal.\n","\n","Calculating the Area under the ROC curve (AUC) is a good indicator of performance (we aim to be close to AUC = 1).\n","\n","Read more about the ROC curve and AUC [here](https://keras.rstudio.com/reference/metric_auc.html)."],"id":"gzZsHwSDclhN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"m350U2IPWgHb"},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(y_test, prob, pos_label=1)\n","auc = roc_auc_score(y_test, prob)\n","\n","\n","figsize=(5,5)\n","fig, axis1 = plt.subplots(figsize=figsize)\n","x_onetoone = y_onetoone = [0, 1]\n","plt.plot(fpr, tpr, 'r-')\n","plt.plot(x_onetoone, y_onetoone, 'k--',  label=\"1-1\")\n","plt.legend(loc=0)\n","plt.title(\"Receiver Operator Characteristic (ROC)\")\n","plt.xlabel(\"False Positive (1 - Specificity)\")\n","plt.ylabel(\"True Positive (Selectivity)\")\n","#Adding text inside a rectangular box by using the keyword 'bbox'\n","plt.text(0.5, 0.2, \"AUC =\"+\"{0:.2f}\".format(auc), fontsize = 22,\n","         bbox = dict(facecolor = 'red', alpha = 0.5))\n","plt.tight_layout()"],"id":"m350U2IPWgHb"},{"cell_type":"markdown","metadata":{"id":"pswf6IMzd80E"},"source":["You should always report several different performance metrics next to accuracy.\n","For example precision or purity (recall) of your predictions. See how to implement it [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n","\n","What if your dataset includes two classes, but the data is vert imbalanced (one class represnts most of the dataset). You didn't train you model very well. How would accuracy, precision and recall look like in this case?"],"id":"pswf6IMzd80E"},{"cell_type":"markdown","source":["![](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)"],"metadata":{"id":"7-BGzS7VzARJ"},"id":"7-BGzS7VzARJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_A7qJYeWgWE"},"outputs":[],"source":["# Task: Fin the correct formula to give a value to the accuracy, presicion, recall and f1 variables.\n","# Hint: Use the figure above\n","\n","# accuracy:\n","accuracy = ???\n","print('Accuracy: %f' % accuracy)\n","\n","# precision\n","precision = ???\n","print('Precision: %f' % precision)\n","\n","# recall:\n","recall = ???\n","print('Recall: %f' % recall)\n","\n","# f1:\n","f1 = ???\n","print('F1 score: %f' % f1)"],"id":"v_A7qJYeWgWE"},{"cell_type":"markdown","metadata":{"id":"0591d8de"},"source":["## FAQs\n","\n","- **How do I interpret these results?** Observe how the loss for the validation set is higher than for the training set (and conversely, the accuracy for the validation set is lower than for the training set), suggesting that this model is suffering from [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit). Revisit [the original paper](https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract) and notice the strategies they employ to improve the validation accuracy. Observe [their Figure 2](https://www.sciencedirect.com/science/article/pii/S2213133720300445) for an example of what the results of a properly-trained network look like!\n","\n","\n","- **Can I improve the model by changing it?** We only trained for 20 epochs, which is many fewer than the published model. Go back to Section 4 (\"Train the CNN to perform a classification task\") and increase the number of epochs to 100 (or more!) and train again. Does your model perform better? Your results may look better/worse/different from the published results due to the stochastic nature of training.\n","\n","\n","- **Can I try a different model?  I think the results could be improved.** Yes!  You can try adding layers, swapping out the max pooling, changing the activation functions, swapping out the loss function, or trying a different optimizer or learning rate.  Experiment and see what model changes give the best results. You should be aware:  when you start training again, you pick up where your model left off.  If you want to \"reset\" your model to epoch 0 and random weights, you should run the cells to make and compile the model again.\n","\n","\n","- **I want to test my model on my training data!** No. You will convince yourself that your results are much better than they actually are.  Always keep your training, validation, and testing sets completely separate!\n"],"id":"0591d8de"},{"cell_type":"markdown","metadata":{"id":"4aee2cf0"},"source":["### Extensions/Exercises\n","\n","- **Train for longer** Try loading just half of the dataset and train for longer (change the number of epochs to 100 or more). How much did that change the rest of the notebook? Do you get more precise plots? Does changing the number of epochs from 100 to 500 make a huge difference?\n","\n","\n","\n","- **Effect of noise?** Try re-training the network with \"noisy\" data (i.e., modify the `version` in Section 1 to \"noisy\" and download the associated data product). Do the results change? If so, how and why? What are the pros and cons of using noisy vs. pristine data to train a ML model?\n","\n","\n","\n","- **Effect of wavelength?** The DEEPMERGE HLSP includes mock galaxy images in 2 filters only (only HST data). If you train the network with this data (hint: this will require downloading it from the website, or modifying the download cells to point to the correct URL; and also modifying the shapes of the training, validation and test data, as well as the network inputs), how do the results change?\n","\n","\n","\n","- **How good are the model predictions** Use Keras' `model.predict` to generate model predictions for your test set (i.e., the set of data not used to train or validate the model). How do the predictions compare to the \"true\" answers for the test set? Does this comparison improve after increasing the number of epochs, changing the model structure and/or hyperparameters?\n","\n","\n","\n","- **Early stopping?** The DeepMerge team employed \"early stopping\" to minimize overfitting. Try implementing it in the network here! The Keras library for [early stopping](https://keras.io/api/callbacks/early_stopping/) functions will be useful. For example, you can recompile the model, train for many more epochs, and include a `callback`, in `model.train` e.g.,\n","\n","    `callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)`\n","    \n","\n","*Don't forget, [the DeepMerge team provides code](https://github.com/deepskies/deepmerge-public) for building their production-level model and verifying its results, please check them out for more extensions and ideas!*\n"],"id":"4aee2cf0"},{"cell_type":"markdown","metadata":{"id":"3b2c2acc"},"source":["## About this Notebook\n","This notebook has been slightly modified by A. ƒÜiprijanoviƒá and is based on the notebook hoseted on the [Helo Universe website](https://archive.stsci.edu/hello-universe). Some modifications are also utilizing amazing explanations and images from a [tutorial by John Wu](https://github.com/DataDrivenGalaxyEvolution/galevo23-tutorials/tree/main/week-1).\n","\n","**Author of the orignal notebook:** Claire Murray, Data Scientist, cmurray1@stsci.edu based on the code repository for the paper <a href=\"https://www.sciencedirect.com/science/article/pii/S2213133720300445#fn3\">\"DeepMerge: Classifying High-redshift Merging Galaxies with Deep Neural Networks\"</a>, A. ƒÜiprijanoviƒá, G.F. Snyder, B. Nord, J.E.G. Peek, Astronomy & Computing, Volume 32, July 2020, and the notebook \"CNN_for_cluster_masses\" by Michelle Ntampaka, Data Scientist, mntampaka@stsci.edu.\n","\n","**Updated On:** 2022-3-10"],"id":"3b2c2acc"},{"cell_type":"markdown","metadata":{"id":"3d60053a"},"source":["## Citations\n","\n","If you use this data set, `astropy`, or `keras` for published research, please cite the\n","authors. Follow these links for more information:\n","\n","* [Citing the data set](https://www.sciencedirect.com/science/article/pii/S2213133720300445#fn3)\n","* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n","* [Citing `keras`](https://keras.io/getting_started/faq/#how-should-i-cite-keras)\n","\n"],"id":"3d60053a"},{"cell_type":"markdown","metadata":{"id":"d10703b3"},"source":["[Top of Page](#top)"],"id":"d10703b3"}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}