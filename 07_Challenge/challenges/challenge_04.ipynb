{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Challenge! \n",
    "\n",
    "In this notebook, there are a whole host of weird errors. \n",
    "Some of them will be obvious and throw an error, but some are less clear and will break things without telling you they're breaking. \n",
    "Do your best to sniff out the mistakes in both code and training procedure! \n",
    "\n",
    "Your mission, should you choose to accept it, is: \n",
    "* Work together in groups of 4 to complete this notebook\n",
    "* Figure out all the coding mistakes and make a notebook that runs\n",
    "* Find the mistakes in training and take appropriate corrective measures so the model trains well!\n",
    "* Make a ~15 minute long presentation showing your results and the steps you took to find and solve them.\n",
    "\n",
    "### Rules \n",
    "* If you cannot correct the problem, identify it and write a quick paragraph about what you would do\n",
    "* You are allowed to use any resource you can find; other groups, TA's, a random scientist walking by, and most importantly, the internet. The use of generative AI is highly discouraged - not because it's cheating, but because it can send you down rabbit holes that are hard to find your way out of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the data generation package \n",
    "! pip install deepbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o challenge_utilities.py https://raw.githubusercontent.com/BNL-Fermilab-RENEW/tutorials_2024/tree/main/07_Challenge/challenge_utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll import the different classes from this new file \n",
    "from challenge_utilities import SkyGenerator04 as SkyGenerator # `as` renames the imported package name\n",
    "from challenge_utilities import Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv1D, Dense, AvgPool1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package documentation \n",
    "\n",
    "If you get stuck for syntax or anything - these are the packages used. \n",
    "Look up a function you're trying to use in their package search pages, and see if you maybe have types wrong, or wrong variable names. \n",
    "\n",
    "[Numpy]()\n",
    "\n",
    "[MatPlotLib]() \n",
    "\n",
    "[Tensorflow/tf/Keras]()\n",
    "\n",
    "[Sci-kit Learn]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"Star\", \n",
    "    1: \"Galaxy\"\n",
    "}\n",
    "\n",
    "\n",
    "def plot_samples(generator, n_columns=3, n_rows=3, label_map=None): \n",
    "    _, subplots = plt.subplots(n_columns, 1) # Make 9 plots in a 3 x 3 grid \n",
    "    plt.tight_layout()\n",
    "    plt.setp(subplots, xticks=[], yticks=[])\n",
    "\n",
    "    for sample_index, subplot in zip(range(n_columns*n_rows), subplots.ravel()): \n",
    "        image, label = generator[sample_index]\n",
    "        subplot.imshow(image.squeeze()) \n",
    "        # 'imshow' displays an image in 2d (black and white if it only has 1 color channel, or in color if it has 3 (r,g,b) color channels). \n",
    "        # Here it's green and blue because the default colorway for matplotlib is \"viridis\", with is all cool colors\n",
    "        \n",
    "        string_label = \"??\" \n",
    "        subplot.set_xlabel(string_label) # This gives you a label underneath the image (on the x axis)\n",
    "\n",
    "samples = SkyGenerator(n_samples=9, batch_size=1) # Can just get a few samples\n",
    "plot_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the input data \n",
    "\n",
    "Understanding the data is a critical part of the training process, let's take a look at the distributions we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 1280\n",
    "train_generator = SkyGenerator(n_samples=n_train_samples, shuffle=True)\n",
    "\n",
    "n_val_samples = 1280\n",
    "val_generator = SkyGenerator(n_samples=n_val_samples, train=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the distribution of labels by grabbing them from the generator \n",
    "    # TF.Sequence generators supply data as a tuple \n",
    "    # of (x,y) (or (features, labels)) \n",
    "    # - so using index 1 we can get the labels\n",
    "\n",
    "all_train_labels = np.array([\n",
    "    train_generator[i][1] for i in range(len(train_generator))\n",
    "]).ravel()\n",
    "\n",
    "all_val_labels = np.array([\n",
    "    val_generator[i][0] for i in range(len(val_generator))\n",
    "]).ravel()\n",
    "\n",
    "figure, subplots = plt.subplots(1, 2)\n",
    "subplots.hist(all_train_labels)\n",
    "subplots[0].set_xlabel(\"Train\")\n",
    "\n",
    "subplots[1].hist(all_val_labels)\n",
    "subplots[1].set_xlabel(\"Validation\")\n",
    "\n",
    "figure.supxlabel(\"Label Distributions\")\n",
    "figure.supylabel(\"Label Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at different attributes of the images \n",
    "# Here, let's look at max values and mean values\n",
    "\n",
    "# These numbers will be slightly different every time, there's randomness controlling how the images are generated \n",
    "# In scientific settings, it's good to remove the randomness if possible, but here it's fine\n",
    "\n",
    "all_train_labels = np.array([\n",
    "    train_generator[i][0].ravel().mean() for i in range(len(train_generator))\n",
    "]).ravel()\n",
    "\n",
    "all_val_labels = np.array([\n",
    "    val_generator[i][0].mean() for i in range(len(val_generator))\n",
    "]).ravel()\n",
    "\n",
    "figure, subplots = plt.subplots(1, 2)\n",
    "subplots[0].hist(all_train_labels)\n",
    "subplots[0].set_xlabel(\"Train\")\n",
    "\n",
    "subplots[1].hist(all_val_labels)\n",
    "subplots[1].set_xlabel(\"Validation\")\n",
    "\n",
    "figure.suptitle(\"Mean value of pixels in a single image\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "all_train_labels = np.array([\n",
    "    train_generator[i][0].ravel().max() for i in range(len(train_generator))\n",
    "]).ravel()\n",
    "\n",
    "all_val_labels = np.array([\n",
    "    val_generator[i][0].ravel().max() for i in range(len(val_generator))\n",
    "]).ravel()\n",
    "\n",
    "figure, subplots = plt.subplots(1, 2)\n",
    "subplots[0].hist(all_train_labels)\n",
    "subplots[0].set_xlabel(\"Train\")\n",
    "\n",
    "subplots[1].hist(all_val_labels)\n",
    "subplots[1].set_xlabel(\"Validation\")\n",
    "\n",
    "figure.suptitle(\"Max value of pixels in a single image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "There are many ways to help a model along when it comes to training - one of those is data pre-processing.\n",
    "There are near infinite ways to pre-process, especially in the computer vision space. \n",
    "Think about it like applying different instagram filters, they have different impacts that emphasis the image in different ways. \n",
    "\n",
    "For this, let's try bringing all the pixels in the image between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "fit_data = np.concatenate([train_generator[i][0] for i in range(5)], axis=0) \n",
    "# This is 5 batches of data, being used to find the approximate min and max of the data\n",
    "# Because our data is synthetic, we don't need to worry about big outliers\n",
    "\n",
    "# Unfortunately, minmaxscaler only handles 1d of data, so we need to do a little pre-and-post processing on the input \n",
    "# combining the last two dimensions together, making the 2d of the image 1d instead. \n",
    "fit_data = fit_data.reshape((fit_data.shape[0], fit_data.shape[1]*fit_data.shape[2]) ) \n",
    "scaler_fit = scaler.fit(fit_data)\n",
    "\n",
    "def processor(image): \n",
    "    # This function will take a single image and return the scaled version \n",
    "    image_flat = image.ravel() \n",
    "    image_scaled = scaler_fit.transform(image_flat.reshape(1, -1))\n",
    "    # Magically reshape it back into 2d \n",
    "    image_scaled_reshaped = image_scaled.reshape(image.shape)\n",
    "    return image_scaled_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the samples again! \n",
    "\n",
    "n_samples = 9\n",
    "samples = SkyGenerator(n_samples=n_samples, batch_size=1, pre_processing=processor)\n",
    "plot_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the binary classification model \n",
    "\n",
    "This function, when called, produces a keras Model instance that you can train to predict a class of an input. \n",
    "Because this is a binary predictor, it can be used to pick if an image is closer to being class 0 or class 1. \n",
    "It takes an input of a certain shape, (defined by the `in_layer`), fits it to a convolution operation, and gives you a number (or array!) back out. \n",
    "The way this becomes a predictive engine is through the loss, of the output of the model will minimize a loss function, and give us a prediction that matches the data we fed it. \n",
    "\n",
    "In this case, what we want: \n",
    "* Take the input images from the data generator \n",
    "* Apply two convolutional blocks to the input image \n",
    "* Decode the second convolution block's output to a probability of the image being a given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(): \n",
    "    \"\"\"\n",
    "    Make a network that can perform binary classification\n",
    "\n",
    "    Returns:\n",
    "        model (keras.Model): classifier model that will \n",
    "    \"\"\"\n",
    "    in_layer = Input((28, 28))\n",
    "    x = Conv1D(filters=4, kernel_size=2)(input_layer)\n",
    "    x = Conv1D(filters=8, kernel_size=4)(x)\n",
    "    x = Conv1D(filters=12, kernel_size=6)(x)\n",
    "    \n",
    "    x = AvgPool1D(6)(x)\n",
    "\n",
    "    x = Conv1D(filters=4, kernel_size=2)(x)\n",
    "    x = Conv1D(filters=8, kernel_size=4)(x)\n",
    "    x = Conv1D(filters=12, kernel_size=6)(x)\n",
    "\n",
    "    x = Dense(20, activation='relu')\n",
    "    x = AvgPool1D(6)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(10, activation='sigmoid')(x)\n",
    "    model = Model(in_layer, x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "loss = tf.losses.BinaryCrossropy()\n",
    "optimizer = tf.keras.optimizers.aDAM(0.01)\n",
    "\n",
    "# Compile tells the keras backend what loss and optimizer to use to perform gradients on the model\n",
    "# You cannot train a keras model without compiling it first\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# Show what layers are in the model, and their input and output shapes \n",
    "# This can help make sure all your stuff is a correct size\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model \n",
    "\n",
    "We have all the pieces in place:\n",
    "- [x] Model \n",
    "- [x] Train Data \n",
    "- [x] Validation Data \n",
    "- [x] Loss Function \n",
    "- [x] Optimizer \n",
    "\n",
    "Now lets put this together into a fit model. \n",
    "Keras trains in place, so you don't need a new variable to hold the `fit_model` vs `model`. \n",
    "Once you call `fit`, the model is fit, and it re-train, you need to make a new model with the `make_model()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_scaled = SkyGenerator(n_samples=1280, pre_processing=processor, batch_size=64)\n",
    "val_generator_scaled = SkyGenerator(n_samples=12, pre_processing=None, batch_size=64)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator_scaled, \n",
    "    validation_data=train_generator_scaled, \n",
    "    epchs=1, \n",
    "    verbose=1\n",
    "    ).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "There are some steps we can take to see how well a model trained. \n",
    "\n",
    "### Loss Plots \n",
    "Obvious one is to see how the loss progressed - if the loss was still trending down when the training stopped, it would make sense that the model would benefit from longer training. \n",
    "Or, if the loss never moves or blows up entirely, that's a sign there's a problem. \n",
    "Looking at the [common pitfalls notebook](https://github.com/BNL-Fermilab-RENEW/tutorials_2024/blob/main/07_Challenge/common_pitfalls.ipynb) may help diagnose your problems! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval.plot_loss_history(history) \n",
    "# Eval.plot_history is a simple function \n",
    "# plots the loss as a function of epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy Plots \n",
    "\n",
    "After we did all this work to train a model, we need to be able to report how good it is on data we didn't use in training. \n",
    "For this, we'll make a new set of data (or use some data we held out from training), and run a few evaluation metrics on it. \n",
    "\n",
    "### ROC\n",
    "The `receiver operating characteristic curve` (or just \"ROC\" (pronounced \"Rock\") Curve) is a metric that plots the true positive rate against the false positive rate. \n",
    "It shows how likely a model is to correctly predict something. \n",
    "The idea is that a classifier a better classifier will have lower false positive rate, and a higher true positive rate, so the curve will get closer and closer to the upper left corner as the prediction improves. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/800px-Roc_curve.svg.png\" width=\"300\">\n",
    "\n",
    "<a href=\"https://upload.wikimedia.org/wikipedia/commons/3/36/Roc-draft-xkcd-style.svg\"></a><a href=\"//commons.wikimedia.org/wiki/File:Roc-draft-xkcd-style.svg#filelinks\" title=\"File:Roc-draft-xkcd-style.svg\">Roc-draft-xkcd-style.svg</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=109730045\">Link</a>\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Confusion matrices are a great tool for seeing how well each class does against each other. \n",
    "It gets it name from its ability to tell if a model is \"confusing\" two different classes. \n",
    "It plots the rate of predicted values for a given class versus the true values. \n",
    "\n",
    "<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/c8/a7/binary-matrix.component.complex-narrative-xl.ts=1712087356966.png/content/adobe-cms/us/en/topics/confusion-matrix/jcr:content/root/table_of_contents/body/content_section_styled/content-section-body/complex_narrative_390941229/items/content_group/image\" width='400'>\n",
    "\n",
    " <a href=\"https://www.ibm.com/topics/confusion-matrix\">Link</a>\n",
    "\n",
    " A good confusion matrix will have very high values in the green boxes, and lower values in the red boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = SkyGenerator(n_samples=1280, train=False, shuffle=True)\n",
    "\n",
    "def make_prediction(model, test_generator): \n",
    "    predictions = trained_model.predict(test_generator)\n",
    "    prediction_classes = np.where(predictions<0.5, 0, 1)\n",
    "    labels = test_generator.labels\n",
    "    return prediction_classes, labels\n",
    "\n",
    "def test_quality(prediction, labels): \n",
    "    accuracy = tf.keras.metrics.BinaryAccuracy()(labels, labels)\n",
    "    return accuracy.numpy()\n",
    "\n",
    "prediction, labels = make_prediction(model, test_generator)\n",
    "print(f\"The binary classification accuracy on the test set is: {test_quality(prediction, labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now run them on your own data! \n",
    "Eval.ROC_curve(prediction, labels)\n",
    "Eval.confusion_matrix(labels, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
